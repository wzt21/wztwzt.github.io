残差神经网络：

<img width="650" height="365" alt="Image" src="https://github.com/user-attachments/assets/938de5ce-232c-4bf9-a725-c2ac03d050fa" />

深度残差学习

F(x)+x 的公式可以通过具有“快捷连接”的前馈神经网络实现（图 2）。快捷连接 [2, 34, 49] 是那些跳过一个或多个层的连接。在我们的情况下，快捷连接只是执行恒等映射，并且它们的输出被添加到堆叠层的输出（图 2）。恒等快捷连接既不添加额外的参数也不添加计算复杂度。整个网络仍然可以通过 SGD 和反向传播进行端到端训练，并且可以使用常见的库（例如 Caffe [19]）轻松实现，而无需修改求解器。

如果添加的层可以被构造为恒等映射，那么更深模型应该具有不高于其更浅对应架构的训练误差。退化问题表明求解器可能难以通过多个非线性层来近似恒等映射。

在本文中，我们考虑一个定义如下所示的构建块：
<img width="214" height="51" alt="Image" src="https://github.com/user-attachments/assets/5acec833-eac2-4a3d-834b-4456ef534cfe" />
这里 x 和 y 是考虑的层的输入和输出向量。函数 F(x, {Wi}) 表示要学习的残差映射。对于图 2 中的示例，它具有两个层，F = W2σ(W1x) 其中 σ 表示 ReLU [29]，并且省略了偏置以简化符号。F + x 的操作由快捷连接和逐元素加法执行。我们采用第二个非线性层后进行加法（即，σ(y)，见图 2）。公式（1）中的快捷连接既不引入额外的参数也不引入计算复杂度。这在实践中很有吸引力，而且在我们对普通网络和残差网络的比较中也很重要。我们可以公平地比较同时具有相同数量参数、深度、宽度和计算成本（除了可以忽略不计的逐元素加法）的普通/残差网络。公式（1）中 x 和 F 的维度必须相等。如果不是这种情况（例如，在更改输入/输出通道时），我们可以通过快捷连接执行线性投影 Ws 以匹配维度：
<img width="290" height="54" alt="Image" src="https://github.com/user-attachments/assets/3b50f8fa-f3ad-462e-8cd1-44740dc72eaa" />
我们也可以在公式（1）中使用方阵 Ws。但我们将通过实验表明，恒等映射足以解决退化问题，并且是经济的，因此 Ws 仅在匹配维度时使用。

**残差网络。**基于上述普通网络，我们插入快捷连接（图 3，右），这将网络转换为其对应的残差版本。当输入和输出具有相同维度时，可以直接使用恒等快捷连接（图 3 中的实线快捷连接）。当维度增加时（图 3 中的虚线快捷连接），我们考虑两种选项：（A）快捷连接仍然执行恒等映射，并添加额外的零条目以增加维度。此选项不引入额外的参数；（B）使用公式（2）中的投影快捷连接来匹配维度（通过 1×1 卷积完成）。对于两种选项，当快捷连接跨越两个大小不同的特征图时，它们都以步幅为 2 执行。
<img width="390" height="504" alt="Image" src="https://github.com/user-attachments/assets/86650341-cc57-49c7-a2a8-b5432c1b3e31" />
<img width="386" height="443" alt="Image" src="https://github.com/user-attachments/assets/4cb7e8c3-8995-4877-8ae0-d6c7edd70932" />
